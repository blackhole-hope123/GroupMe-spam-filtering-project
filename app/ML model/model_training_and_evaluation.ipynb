{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0ce6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\smile\\anaconda3\\envs\\erdos_summer_2025\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from transformers.utils import logging\n",
    "logging.set_verbosity_info()\n",
    "import tqdm\n",
    "tqdm.tqdm.write(\"\")  # Forces initialization\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8683270d",
   "metadata": {},
   "source": [
    "Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f471e5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "DATA_FOLDER=\"refined_labels\"\n",
    "DATA_FILE=\"final_labels.json\"\n",
    "subfolders=[f for f in os.listdir(DATA_FOLDER) if os.path.isdir(os.path.join(DATA_FOLDER, f))]\n",
    "for subfolder in subfolders:\n",
    "    subfolder_path=os.path.join(DATA_FOLDER,subfolder)\n",
    "    data_file_path=os.path.join(subfolder_path,DATA_FILE)\n",
    "    with open(data_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.concat([pd.DataFrame(data),df],axis=0)\n",
    "\n",
    "\n",
    "# clean data\n",
    "df = df[df[\"label\"] != \"skip\"]\n",
    "df = df[[\"message\", \"label\"]]\n",
    "df[\"label\"] = df[\"label\"].map({\"spam\": 1, \"ham\": 0})\n",
    "\n",
    "\n",
    "X = df[\"message\"]\n",
    "y = df[\"label\"]\n",
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle = True)\n",
    "X_train, X_val, y_train, y_val =  train_test_split(X_trainval, y_trainval, test_size=0.125, random_state=41, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d937e8",
   "metadata": {},
   "source": [
    "The Baseline Model (Naive Bayes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a1cb805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Naive Bayes =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       544\n",
      "           1       1.00      0.43      0.60        21\n",
      "\n",
      "    accuracy                           0.98       565\n",
      "   macro avg       0.99      0.71      0.79       565\n",
      "weighted avg       0.98      0.98      0.97       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "baseline = MultinomialNB()\n",
    "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=2000)\n",
    "pipeline = make_pipeline(vectorizer, baseline)\n",
    "pipeline.fit(X_train, y_train)\n",
    "y_pred = pipeline.predict(X_val)\n",
    "\n",
    "print(f\"\\n===== {\"Naive Bayes\"} =====\")\n",
    "print(classification_report(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3e3685",
   "metadata": {},
   "source": [
    "Model selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bae52a4",
   "metadata": {},
   "source": [
    "Some simple models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bce41a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Naive Bayes =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       544\n",
      "           1       1.00      0.43      0.60        21\n",
      "\n",
      "    accuracy                           0.98       565\n",
      "   macro avg       0.99      0.71      0.79       565\n",
      "weighted avg       0.98      0.98      0.97       565\n",
      "\n",
      "\n",
      "===== Random Forest =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       544\n",
      "           1       0.94      0.81      0.87        21\n",
      "\n",
      "    accuracy                           0.99       565\n",
      "   macro avg       0.97      0.90      0.93       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      "\n",
      "\n",
      "===== Linear SVM =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       544\n",
      "           1       1.00      0.76      0.86        21\n",
      "\n",
      "    accuracy                           0.99       565\n",
      "   macro avg       1.00      0.88      0.93       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = {\n",
    "    \"Naive Bayes\": MultinomialNB(),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=200, random_state=42),\n",
    "    \"Linear SVM\": LinearSVC()\n",
    "}\n",
    "\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = make_pipeline(vectorizer, model)\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_val)\n",
    "\n",
    "    print(f\"\\n===== {name} =====\")\n",
    "    print(classification_report(y_val, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36731b55",
   "metadata": {},
   "source": [
    "Both Linear SVM and Random Forest behaves better than the baseline model. However, recall is more important in spam filtering, so Linear SVM behaves better than Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e31d3b",
   "metadata": {},
   "source": [
    "More advance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edfa9b35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "bert_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be684ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        self.encodings = tokenizer(list(texts), truncation=True, padding=True, max_length=128)\n",
    "        self.labels = list(labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c01f2675",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TextDataset(X_train, y_train)\n",
    "val_dataset = TextDataset(X_val, y_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5739923",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_output\",\n",
    "    num_train_epochs=2,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=2e-5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9eb4887a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 3,955\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 990\n",
      "  Number of trainable parameters = 109,483,778\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "827db786ab2743cc89ef3ef48d2d77f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/990 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "350c89e3caaa4706b49b91eab8490c65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.07336991280317307, 'eval_runtime': 106.0695, 'eval_samples_per_second': 5.327, 'eval_steps_per_second': 0.669, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./bert_output\\checkpoint-500\n",
      "Configuration saved in ./bert_output\\checkpoint-500\\config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1052, 'grad_norm': 0.045111317187547684, 'learning_rate': 9.8989898989899e-06, 'epoch': 1.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./bert_output\\checkpoint-500\\model.safetensors\n",
      "Saving model checkpoint to ./bert_output\\checkpoint-990\n",
      "Configuration saved in ./bert_output\\checkpoint-990\\config.json\n",
      "Model weights saved in ./bert_output\\checkpoint-990\\model.safetensors\n",
      "\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 565\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14d65f764bdc47fdb26b79e1eaf64c9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.04849190637469292, 'eval_runtime': 97.8404, 'eval_samples_per_second': 5.775, 'eval_steps_per_second': 0.726, 'epoch': 2.0}\n",
      "{'train_runtime': 5706.3823, 'train_samples_per_second': 1.386, 'train_steps_per_second': 0.173, 'train_loss': 0.08285411873249092, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=990, training_loss=0.08285411873249092, metrics={'train_runtime': 5706.3823, 'train_samples_per_second': 1.386, 'train_steps_per_second': 0.173, 'total_flos': 520302111974400.0, 'train_loss': 0.08285411873249092, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b04120",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "***** Running Prediction *****\n",
      "  Num examples = 565\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63f4df8a70e943bf8c0d4ffb6bdf3d31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/71 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== BERT Transformer =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       544\n",
      "           1       1.00      0.67      0.80        21\n",
      "\n",
      "    accuracy                           0.99       565\n",
      "   macro avg       0.99      0.83      0.90       565\n",
      "weighted avg       0.99      0.99      0.99       565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred = trainer.predict(val_dataset)\n",
    "y_pred_bert = pred.predictions.argmax(axis=-1)\n",
    "print(\"\\n===== BERT Transformer =====\")\n",
    "print(classification_report(y_val, y_pred_bert))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e731cb",
   "metadata": {},
   "source": [
    "The bert model performs no better than the linear SVM model, probably because the dataset is not large enough. We adopt the linear SVM model as our final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8a11b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== Linear SVM =====\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99      1083\n",
      "           1       0.95      0.79      0.86        47\n",
      "\n",
      "    accuracy                           0.99      1130\n",
      "   macro avg       0.97      0.89      0.93      1130\n",
      "weighted avg       0.99      0.99      0.99      1130\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_model = LinearSVC()\n",
    "final_pipeline = make_pipeline(vectorizer, final_model)\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "\n",
    "print(f\"\\n===== {\"Linear SVM\"} =====\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03fc7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline saved successfully!\n"
     ]
    }
   ],
   "source": [
    "# Save the pipeline to a file\n",
    "joblib.dump(final_pipeline, \"groupme_spam_pipeline.pkl\")\n",
    "print(\"Pipeline saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "erdos_summer_2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
